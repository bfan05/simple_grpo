#!/usr/bin/env bash
set -euo pipefail

echo "=============================="
echo " RL BOX SETUP (DLAMI + CONDA)"
echo "=============================="

ENV_NAME="${ENV_NAME:-rl}"
PYTHON_VERSION="${PYTHON_VERSION:-3.10}"

# Default to cu128 on RTX 5090 / Blackwell; allow override via env var.
TORCH_CUDA_INDEX_URL_DEFAULT="https://download.pytorch.org/whl/cu128"
TORCH_CUDA_INDEX_URL="${TORCH_CUDA_INDEX_URL:-$TORCH_CUDA_INDEX_URL_DEFAULT}"

CACHE_ROOT="${CACHE_ROOT:-/mnt/data}"
USE_MNT_DATA="false"
if [ -d "$CACHE_ROOT" ] && mountpoint -q "$CACHE_ROOT"; then
  USE_MNT_DATA="true"
fi

echo "Env: $ENV_NAME | Python: $PYTHON_VERSION"
echo "PyTorch index-url: $TORCH_CUDA_INDEX_URL"
echo "Cache root: $CACHE_ROOT (mounted: $USE_MNT_DATA)"

# -----------------------------
# System packages (optional)
# -----------------------------
echo "[1/9] System packages (optional)..."
if command -v sudo >/dev/null 2>&1; then
  if sudo -n true 2>/dev/null; then
    sudo apt-get update
    sudo apt-get install -y \
      git tmux htop nvtop \
      build-essential \
      curl wget unzip \
      ca-certificates \
      python3-venv
  else
    echo "NOTE: sudo requires a password or isn't allowed. Skipping apt installs."
  fi
else
  echo "NOTE: sudo not available. Skipping apt installs."
fi

# -----------------------------
# NVIDIA sanity check
# -----------------------------
echo "[2/9] Checking NVIDIA driver..."
command -v nvidia-smi >/dev/null 2>&1 || { echo "ERROR: nvidia-smi not found."; exit 1; }
nvidia-smi

# If user didn't override TORCH_CUDA_INDEX_URL, auto-pick based on GPU name.
# RTX 5090 (sm_120) needs cu128+ wheels.
if [ "${TORCH_CUDA_INDEX_URL:-}" = "$TORCH_CUDA_INDEX_URL_DEFAULT" ]; then
  GPU_NAME="$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -n1 || true)"
  if echo "$GPU_NAME" | grep -qi "RTX 5090"; then
    TORCH_CUDA_INDEX_URL="https://download.pytorch.org/whl/cu128"
  fi
fi

echo "Resolved PyTorch index-url: $TORCH_CUDA_INDEX_URL"

# -----------------------------
# Conda availability
# -----------------------------
echo "[3/9] Ensuring conda is available..."
CONDA_DIR="$HOME/miniconda3"

if command -v conda &>/dev/null; then
  echo "Found conda: $(which conda)"
else
  if [ ! -d "$CONDA_DIR" ]; then
    echo "conda not found; installing Miniconda to $CONDA_DIR..."
    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh
    bash /tmp/miniconda.sh -b -p "$CONDA_DIR"
    rm /tmp/miniconda.sh
  fi
fi

# Make conda usable in THIS non-interactive script.
# Do NOT rely on `conda info --base` until conda is actually on PATH.
if [ -f "$CONDA_DIR/etc/profile.d/conda.sh" ]; then
  # shellcheck disable=SC1091
  source "$CONDA_DIR/etc/profile.d/conda.sh"
elif command -v conda &>/dev/null; then
  # shellcheck disable=SC1091
  source "$(conda info --base)/etc/profile.d/conda.sh"
else
  echo "ERROR: conda installed but conda.sh not found and conda not on PATH."
  echo "Try: source \"$CONDA_DIR/etc/profile.d/conda.sh\""
  exit 1
fi

echo "Conda base: $(conda info --base)"
conda --version

# -----------------------------
# Create/activate environment
# -----------------------------
echo "[4/9] Creating/activating conda env..."
if ! conda env list | awk '{print $1}' | grep -qx "$ENV_NAME"; then
  conda create -y -n "$ENV_NAME" python="$PYTHON_VERSION"
fi
conda activate "$ENV_NAME"

python --version
pip install --upgrade pip setuptools wheel

# -----------------------------
# Put caches on big disk (if mounted)
# -----------------------------
echo "[5/9] Configuring caches..."
if [ "$USE_MNT_DATA" = "true" ]; then
  mkdir -p "$CACHE_ROOT/hf" "$CACHE_ROOT/torch" "$CACHE_ROOT/wandb" "$CACHE_ROOT/tmp"

  ENV_SNIPPET="$HOME/.rl_box_env"
  cat > "$ENV_SNIPPET" << EOF
# Generated by setup_box.sh
export HF_HOME="$CACHE_ROOT/hf"
export TRANSFORMERS_CACHE="$CACHE_ROOT/hf/transformers"
export HF_DATASETS_CACHE="$CACHE_ROOT/hf/datasets"
export TORCH_HOME="$CACHE_ROOT/torch"
export WANDB_DIR="$CACHE_ROOT/wandb"
export TMPDIR="$CACHE_ROOT/tmp"
EOF

  if ! grep -q "source \$HOME/.rl_box_env" "$HOME/.bashrc" 2>/dev/null; then
    {
      echo ""
      echo "# RL box env (added by setup_box.sh)"
      echo "source \$HOME/.rl_box_env"
    } >> "$HOME/.bashrc"
  fi

  # shellcheck disable=SC1090
  source "$ENV_SNIPPET"
else
  echo "NOTE: $CACHE_ROOT not mounted. Caches will remain on root disk."
fi

# -----------------------------
# Install PyTorch (CUDA) + core stack
# -----------------------------
echo "[6/9] Installing PyTorch..."
echo "Using: $TORCH_CUDA_INDEX_URL"
pip install --upgrade torch torchvision torchaudio --index-url "$TORCH_CUDA_INDEX_URL"

python - << 'EOF'
import torch
print("Torch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("Torch CUDA:", torch.version.cuda)
    print("GPU:", torch.cuda.get_device_name(0))
    # Force an actual kernel to run:
    x = torch.randn(1024,1024, device="cuda")
    y = x @ x
    print("CUDA matmul ok:", y.mean().item())
EOF

# -----------------------------
# Install HF / TRL stack
# -----------------------------
echo "[7/9] Installing Hugging Face + TRL stack..."
pip install --upgrade \
  transformers datasets accelerate trl peft sentencepiece \
  wandb evaluate numpy scipy packaging rich matplotlib

# bitsandbytes is optional; on very new GPUs it can lag behind.
# Install best-effort so it doesn't break the whole setup.
pip install --upgrade bitsandbytes || echo "bitsandbytes install failed; continuing."

# -----------------------------
# Optional: FlashAttention (best-effort)
# -----------------------------
echo "[8/9] Attempting FlashAttention install (optional, best-effort)..."
pip install flash-attn --no-build-isolation || echo "FlashAttention install failed; continuing."

# -----------------------------
# Accelerate default config
# -----------------------------
echo "[9/9] Writing accelerate config..."
ACCEL_DIR="$HOME/.cache/huggingface/accelerate"
mkdir -p "$ACCEL_DIR"
cat > "$ACCEL_DIR/default_config.yaml" << 'EOF'
compute_environment: LOCAL_MACHINE
distributed_type: NO
mixed_precision: bf16
use_cpu: false
num_processes: 1
num_machines: 1
machine_rank: 0
EOF

echo "=============================="
echo " SETUP COMPLETE"
echo "=============================="
echo "Run:"
echo "  source ~/.bashrc"
echo "  conda activate $ENV_NAME"
echo "  python -c \"import torch; print(torch.cuda.is_available())\""
echo "Tip: tmux new -s grpo"
